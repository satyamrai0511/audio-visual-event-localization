# ğŸ¯ Audio-Visual Event Localization in Videos

A research-driven project to design an **autoregressive model** that integrates both **audio** and **video** signals for **temporal event localization**. By aligning auditory and visual streams, the goal is to detect and classify meaningful events more precisely.

---

## ğŸ“Œ Project Goals

- Build an end-to-end multimodal event localization pipeline.
- Leverage autoregressive modeling to capture temporal dependencies.
- Fuse audio-visual features using attention or cross-modal interaction.
- Achieve accurate event classification and localization in videos.

---

## ğŸ“¦ Project Structure

```text
audio-visual-event-localization/
â”œâ”€â”€ data/                   # Sample videos, audios, and annotations
â”œâ”€â”€ models/                 # Autoregressive and fusion model implementations
â”œâ”€â”€ notebooks/              # EDA and experimentation notebooks
â”œâ”€â”€ outputs/                # Logs, predictions, evaluation results
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing/      # Syncing, trimming, and segmentation logic
â”‚   â”œâ”€â”€ feature_extraction/ # Audio & video feature encoders
â”‚   â”œâ”€â”€ training/           # Training scripts and dataloaders
â”‚   â””â”€â”€ evaluation/         # Evaluation metrics and benchmarking
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md               # Project documentation
â””â”€â”€ LICENSE                 # License file
```

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/satyamrai0511/audio-visual-event-localization.git
cd audio-visual-event-localization
```

### 2. Create and Activate Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸš€ Usage (Coming Soon)

```bash
python src/inference.py --video data/sample_video.mp4 --audio data/sample_audio.wav
```

Example output:

```
Detected Event: "Clapping" from 00:15 to 00:19
Detected Event: "Door Slam" from 01:02 to 01:03
```

---

## ğŸ§  Model Architecture (Planned)

- **Audio Encoder**: Pretrained CNN or transformer-based AST
- **Video Encoder**: CLIP/ViT or 3D CNN
- **Fusion Module**: Cross-attention or multimodal transformer
- **Temporal Model**: Autoregressive decoder for event sequence prediction

---

## ğŸ“Š Evaluation Metrics

- Mean Average Precision (mAP)
- Localization Accuracy (IoU-based)
- F1-Score per event type

---

## ğŸ§ª Future Work

- [ ] Real-time streaming support
- [ ] Web UI for uploading and testing video/audio files
- [ ] Integration with pretrained multimodal transformers

---

## ğŸ¤ Contributing

Have ideas to improve the model or codebase? Fork the repo, create a feature branch, and open a pull request! Contributions are welcome.

---

## ğŸ“„ License

This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.
